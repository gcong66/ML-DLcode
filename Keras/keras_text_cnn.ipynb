{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![]('./image/data_process.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "Figure 1.数据处理\n",
    "<img src=\"./image/data_process.png\" width=\"70%\" height=\"70%\" />\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "Figure 2.textCNN原理\n",
    "<img src=\"image/textCNN.png\" width=\"70%\" height=\"70%\" />\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.卷积层  \n",
    "卷积层输入的是一个表示句子的矩阵，维度为`n*d`，即每句话共有`n`个词，每个词有一个`d`维的词向量表示\n",
    "卷积操作后再使用激活函数激活得到相应的特征`ci`，则卷可以表示为:(使用点乘来表示卷积操作)\n",
    "<img src=\"image/1-5.png\"  width=\"500\" >\n",
    "经过卷积操作之后，可以得到一个n-h+1维的向量c形如\n",
    "<img src=\"image/2-3.png\"  width=\"400\" >\n",
    "**可以使用更多高度不同的卷积核，且每个高度的卷积核多个，得到更多不同特征**\n",
    "\n",
    "2.池化层  \n",
    "**在卷积过程中使用了不同高度的卷积核，使得通过卷积层后得到的向量维度会不一致**，所以在池化层中，使用**1-Max-pooling**对每个特征向量池化成一个值，即抽取每个特征向量的最大值表示该特征，而且认为这个最大值表示的是最重要的\n",
    "\n",
    "3.全连接层  \n",
    "wx+b\n",
    "假设有两层全连接层，第一层可以加上’relu’作为激活函数，第二层则使用softmax激活函数得到属于每个类的概率  \n",
    "如情感分析的正负面时，第二层也可以使用sigmoid作为激活函数，然后损失函数使用对数损失函数’binary_crossentropy’\n",
    "\n",
    "4.**在词向量构造方面可以有以下不同的方式**：\n",
    "\n",
    "CNN-rand: 随机初始化每个单词的词向量通过后续的训练去调整。  \n",
    "```\n",
    "embedding = tf.get_variable(name='embedding', shape=[self.config.vocab_size, self.config.embedding_dim],initializer=self.initializer)\n",
    "```\n",
    "CNN-static: 使用预先训练好的词向量，如word2vec训练出来的词向量，在训练过程中不再调整该词向量。  \n",
    "```\n",
    "embedding = np.load(embedding_file)\n",
    "embed1 = tf.constant(embedding, name='embedding')\n",
    "``` \n",
    "CNN-non-static: 使用预先训练好的词向量，并在训练过程进一步进行调整。  \n",
    "```\n",
    "embedding = np.load(embedding_file)\n",
    "embed2 = tf.Variable(embedding, name='embedding')\n",
    "```\n",
    "CNN-multichannel: 将static与non-static作为两通道的词向量。  \n",
    "```\n",
    "embedding = []\n",
    "embedding.append(tf.nn.embedding_lookup(embed1, inputs))\n",
    "embedding.append(tf.nn.embedding_lookup(embed2, inputs))\n",
    "embed = tf.concat(embedding, axis=-1)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "API：  \n",
    "卷积过程由于只是沿着句子长度方向进行卷积，即只在一个维度卷积所以使用Conv1d。 \n",
    "**Conv1d(filters, kernel_size, activation)**:\n",
    "- filters: 卷积核的个数\n",
    "- kernel_size: 卷积核的宽度\n",
    "- activation: 卷积层使用的激活函数\n",
    "\n",
    "\n",
    "池化过程使用的在一个维度上的池化，使用MaxPooling1D  \n",
    "**MaxPooling1D(pool_size)**:\n",
    "- pool_size: 池化窗口的大小，由于我们要将一个卷积核得到特征向量池化为1个值，所以池化窗口可以设为(句子长度-卷积核宽度+1)\n",
    "\n",
    " \n",
    "池化过程最后还需要对每个值拼接起来，可以使用concatenate函数实现。  \n",
    "**concatenate(inputs, axis)**:\n",
    "- inputs: inputs为一个tensor的list，所以需要将得到1-MaxPooling得到每个值append到list中，并把该list作为inputs参数的输入。\n",
    "- axis: 指定拼接的方向。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Conv1D, MaxPooling1D, Dropout, Input, concatenate\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Model\n",
    "import os\n",
    "import tarfile\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 读取数据\n",
    "数据清洗：去除含有html标签的  \n",
    "分词：此处为英文,不需  \n",
    "去停用词：可以去除”the”、”a”等词,此处没加"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def rm_tags(text):\n",
    "    re_tag = re.compile(r'<[^>]+>')\n",
    "    return re_tag.sub('', text)\n",
    "\n",
    "def read_files(filetype):\n",
    "    \"\"\"\n",
    "    filetype: 'train' or 'test'\n",
    "    return:\n",
    "    all_texts: filetype数据集文本\n",
    "    all_labels: filetype数据集标签\n",
    "    \"\"\"\n",
    "    # 标签1表示正面，0表示负面\n",
    "    all_labels = [1]*12500 + [0]*12500\n",
    "    all_texts = []\n",
    "    file_list = []\n",
    "    path = r'./data/aclImdb/'\n",
    "    # 读取正面文本名\n",
    "    pos_path = path + filetype + '/pos/'\n",
    "    for file in os.listdir(pos_path):\n",
    "        file_list.append(pos_path+file)\n",
    "    # 读取负面文本名\n",
    "    neg_path = path + filetype + '/neg/'\n",
    "    for file in os.listdir(neg_path):\n",
    "        file_list.append(neg_path+file)\n",
    "    # 将所有文本内容加到all_texts\n",
    "    for file_name in file_list:\n",
    "        with open(file_name, encoding='utf-8') as f:\n",
    "            all_texts.append(rm_tags(\" \".join(f.readlines())))\n",
    "    return all_texts, all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfile = tarfile.open(r'./data/aclImdb_v1.tar.gz', 'r:gz')  # r;gz是读取gzip压缩文件\n",
    "result = tfile.extractall('./data/')  # 解压缩文件到data文件夹中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts, train_labels = read_files('train')\n",
    "test_texts, test_labels = read_files('test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 处理成深度学习需要的数据格式\n",
    "tokenizer = Tokenizer(num_words=2000)：创建词典,以空格分词  \n",
    "tokenizer.fit_on_texts()：对输入的文本中的每个词编号：编号是根据词频的，词频越大，编号越小    \n",
    "tokenizer.texts_to_sequences()：每一句话的文本转数字，使用每个词的编号进行编号\n",
    "keras.preprocessing.sequence.pad_sequences()：将每句话补齐到相同长度,左补0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(train_texts, train_labels, test_texts, test_labels):\n",
    "    tokenizer = Tokenizer(num_words=2000)  # 建立一个2000个单词的字典\n",
    "    tokenizer.fit_on_texts(train_texts)\n",
    "    #print(tokenizer.word_index)：得到word2id的词典\n",
    "    \n",
    "    # 对每一句影评文字转换为数字列表，使用每个词的编号进行编号\n",
    "    x_train_seq = tokenizer.texts_to_sequences(train_texts)\n",
    "    x_test_seq = tokenizer.texts_to_sequences(test_texts)\n",
    "    \n",
    "    x_train = sequence.pad_sequences(x_train_seq, maxlen=150)\n",
    "    x_test = sequence.pad_sequences(x_test_seq, maxlen=150)\n",
    "    \n",
    "    y_train = np.array(train_labels)\n",
    "    y_test = np.array(test_labels)\n",
    "    return x_train, y_train, x_test, y_test\n",
    "\n",
    "x_train, y_train, x_test, y_test = preprocessing(train_texts, train_labels, test_texts, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 建立text_cnn模型\n",
    "keras.layers.embeddings.Embedding(vocab_size,embed_size)将每个词编码转换为词向量    \n",
    "卷积尺寸采用2,3,4,5,每种100个卷积核  \n",
    "FC1: 32  \n",
    "FC2:sigmoid  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_cnn(maxlen=150, max_features=2000, embed_size=32):\n",
    "    # Inputs\n",
    "    comment_seq = Input(shape=[maxlen], name='x_seq')\n",
    "\n",
    "    # Embeddings layers\n",
    "    emb_comment = Embedding(max_features, embed_size)(comment_seq)\n",
    "\n",
    "    # conv layers\n",
    "    convs = []\n",
    "    filter_sizes = [2, 3, 4, 5]\n",
    "    for fsz in filter_sizes:\n",
    "        l_conv = Conv1D(filters=100, kernel_size=fsz, activation='relu')(emb_comment)\n",
    "        l_pool = MaxPooling1D(maxlen - fsz + 1)(l_conv)\n",
    "        l_pool = Flatten()(l_pool)\n",
    "        convs.append(l_pool)\n",
    "    merge = concatenate(convs, axis=1)\n",
    "\n",
    "    out = Dropout(0.5)(merge)\n",
    "    output = Dense(32, activation='relu')(out)\n",
    "\n",
    "    output = Dense(units=1, activation='sigmoid')(output)\n",
    "\n",
    "    model = Model([comment_seq], output)\n",
    "    #adam = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 22500 samples, validate on 2500 samples\n",
      "Epoch 1/20\n",
      "22500/22500 [==============================] - 4s 199us/step - loss: 0.5685 - acc: 0.6836 - val_loss: 0.4979 - val_acc: 0.7556\n",
      "Epoch 2/20\n",
      "22500/22500 [==============================] - 3s 153us/step - loss: 0.3449 - acc: 0.8512 - val_loss: 0.3620 - val_acc: 0.8296\n",
      "Epoch 3/20\n",
      "22500/22500 [==============================] - 3s 154us/step - loss: 0.2875 - acc: 0.8796 - val_loss: 0.2786 - val_acc: 0.8832\n",
      "Epoch 4/20\n",
      "22500/22500 [==============================] - 3s 153us/step - loss: 0.2431 - acc: 0.9011 - val_loss: 0.4040 - val_acc: 0.8252\n",
      "Epoch 5/20\n",
      "22500/22500 [==============================] - 3s 153us/step - loss: 0.2079 - acc: 0.9180 - val_loss: 0.3534 - val_acc: 0.8524\n",
      "Epoch 6/20\n",
      "22500/22500 [==============================] - 3s 154us/step - loss: 0.1756 - acc: 0.9335 - val_loss: 0.2908 - val_acc: 0.8876\n",
      "Epoch 7/20\n",
      "22500/22500 [==============================] - 3s 153us/step - loss: 0.1450 - acc: 0.9464 - val_loss: 0.3757 - val_acc: 0.8620\n",
      "Epoch 8/20\n",
      "22500/22500 [==============================] - 3s 153us/step - loss: 0.1216 - acc: 0.9554 - val_loss: 0.3519 - val_acc: 0.8752\n",
      "Epoch 9/20\n",
      "22500/22500 [==============================] - 3s 154us/step - loss: 0.1046 - acc: 0.9630 - val_loss: 0.4776 - val_acc: 0.8492\n",
      "Epoch 10/20\n",
      "22500/22500 [==============================] - 3s 153us/step - loss: 0.0813 - acc: 0.9734 - val_loss: 0.4898 - val_acc: 0.8552\n",
      "Epoch 11/20\n",
      "22500/22500 [==============================] - 3s 153us/step - loss: 0.0651 - acc: 0.9780 - val_loss: 0.5325 - val_acc: 0.8544\n",
      "Epoch 12/20\n",
      "22500/22500 [==============================] - 3s 154us/step - loss: 0.0577 - acc: 0.9820 - val_loss: 0.6246 - val_acc: 0.8380\n",
      "Epoch 13/20\n",
      "22500/22500 [==============================] - 3s 152us/step - loss: 0.0490 - acc: 0.9839 - val_loss: 0.7380 - val_acc: 0.8296\n",
      "Epoch 14/20\n",
      "22500/22500 [==============================] - 3s 153us/step - loss: 0.0462 - acc: 0.9849 - val_loss: 0.6377 - val_acc: 0.8552\n",
      "Epoch 15/20\n",
      "22500/22500 [==============================] - 3s 153us/step - loss: 0.0386 - acc: 0.9873 - val_loss: 0.7185 - val_acc: 0.8544\n",
      "Epoch 16/20\n",
      "22500/22500 [==============================] - 3s 153us/step - loss: 0.0373 - acc: 0.9870 - val_loss: 0.7960 - val_acc: 0.8364\n",
      "Epoch 17/20\n",
      "22500/22500 [==============================] - 3s 153us/step - loss: 0.0330 - acc: 0.9892 - val_loss: 0.7787 - val_acc: 0.8440\n",
      "Epoch 18/20\n",
      "22500/22500 [==============================] - 3s 153us/step - loss: 0.0321 - acc: 0.9896 - val_loss: 0.7172 - val_acc: 0.8568\n",
      "Epoch 19/20\n",
      "22500/22500 [==============================] - 3s 154us/step - loss: 0.0335 - acc: 0.9890 - val_loss: 0.9572 - val_acc: 0.8212\n",
      "Epoch 20/20\n",
      "22500/22500 [==============================] - 3s 153us/step - loss: 0.0290 - acc: 0.9900 - val_loss: 0.8517 - val_acc: 0.8436\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2c8cfb28908>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = text_cnn()\n",
    "batch_size = 128\n",
    "epochs = 20\n",
    "model.fit(x_train, y_train,\n",
    "          validation_split=0.1,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 预测模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 3s 116us/step\n",
      "test_loss: 0.693744, accuracy: 0.859160\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(x_test, y_test)\n",
    "print('test_loss: %f, accuracy: %f' % (scores[0], scores[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型训练过程batch_size设为64，epochs为20，最终可以在验证集可以得到85.9%的准确率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
